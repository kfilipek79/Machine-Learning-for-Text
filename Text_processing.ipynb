{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources\n",
    "1. Raw Text\n",
    "2. Defining raw text inside a variable\n",
    "3. NLTK corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.2.5.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 649kB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/18/9c/1f/276bc3f421614062468cb1c9d695e6086d0c73d67ea363c501\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2.5\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read the file \n",
    "def fileread():\n",
    "    file_contents=open('test.txt','r').read()\n",
    "    return file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test file for learning nltk. I hope i become an expert in nltk. I know it would need a lot of efforts from my end but i really want to do this. This would add lot of confidence in me to play a role of DS'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call the function\n",
    "fileread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_text():\n",
    "    text=\"This is a variable that contains some text. It is very easy to write this function\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a variable that contains some text. It is very easy to write this function'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Output of raw text file----------- \n",
      " \n",
      "this is a test file for learning nltk. I hope i become an expert in nltk. I know it would need a lot of efforts from my end but i really want to do this. This would add lot of confidence in me to play a role of DS\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    print (\"\")\n",
    "    print(\"----------Output of raw text file----------- \")\n",
    "    print( \" \")\n",
    "    filecontent=fileread()\n",
    "    print(filecontent)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "=====Output of text file=======\n",
      " \n",
      "This is a variable that contains some text. It is very easy to write this function\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print(\" \")\n",
    "    print(\"=====Output of text file=======\")\n",
    "    print(\" \")\n",
    "    \n",
    "    content=local_text()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowercase conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to convert case of text to lowercase\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "=====Output in lowercase=======\n",
      " \n",
      "this is a test statmenent\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print(\" \")\n",
    "    print(\"=====Output in lowercase=======\")\n",
    "    print(\" \")\n",
    "    \n",
    "    content=lowercase(\"This is a test statMEnent\")\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " -----Tokenized Sentences-----\n",
      " \n",
      "----Content read ...tokenization started----\n",
      " ----- Tokenization completed -----\n",
      "4\n",
      "['this is a test file for learning nltk.', 'I hope i become an expert in nltk.', 'I know it would need a lot of efforts from my end but i really want to do this.', 'This would add lot of confidence in me to play a role of DS']\n"
     ]
    }
   ],
   "source": [
    "#create function to tokenize sentences\n",
    "if __name__=='__main__':\n",
    "    print(\" \")\n",
    "    print(\" -----Tokenized Sentences-----\")\n",
    "    print(\" \")\n",
    "    \n",
    "    file_content=fileread()\n",
    "    \n",
    "    print(\"----Content read ...tokenization started----\")\n",
    "    \n",
    "    sent_tokens=sent_tokenize(file_content)\n",
    "    \n",
    "    print(\" ----- Tokenization completed -----\")\n",
    "    print(len(sent_tokens))\n",
    "    print(sent_tokens)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " -----Tokenized Sentences-----\n",
      " \n",
      "----Content read ...tokenization started----\n",
      " ----- Tokenization completed -----\n",
      "2\n",
      "['This is a variable that contains some text.', 'It is very easy to write this function']\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print(\" \")\n",
    "    prnt(\" -----Tokenized Sentences-----\")\n",
    "    print(\" \")\n",
    "    \n",
    "    file_content=local_text()\n",
    "    \n",
    "    print(\"----Content read ...tokenization started----\")\n",
    "    \n",
    "    sent_tokens=sent_tokenize(file_content)\n",
    "    \n",
    "    print(\" ----- Tokenization completed -----\")\n",
    "    print(len(sent_tokens))\n",
    "    print(sent_tokens)\n",
    "    i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(x):\n",
    "    port=PorterStemmer()\n",
    "    return \" \".join([port.stem(i) for i in x.split()])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thi is a trial base program'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming(\"This is a trial based programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " -----Tokenized Sentences-----\n",
      " \n",
      "----Content read ...stemming started----\n",
      "['this is a test file for learning nltk', ' i hope i become an expert in nltk', ' i know it would need a lot of efforts from my end but i really want to do thi', ' this would add lot of confidence in me to play a role of d']\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print(\" \")\n",
    "    print(\" -----Tokenized Sentences-----\")\n",
    "    print(\" \")\n",
    "    \n",
    "    file_content=fileread()\n",
    "    \n",
    "    print(\"----Content read ...stemming started----\")\n",
    "    port=PorterStemmer()\n",
    "    \n",
    "    stemmed_doc=[]\n",
    "    for i in file_content.split(\".\"):\n",
    "        stemmed_doc.append(\"\".join(port.stem(i)))\n",
    "    \n",
    "    print( stemmed_doc )\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordlist():\n",
    "    sw=stopwords.words(\"english\")\n",
    "    \n",
    "       \n",
    "        \n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    words=stopwordlist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "----Reading Content of raw text-----\n",
      " \n",
      "{'I', 'would', 'this'}\n",
      "['test', 'file', 'learning', 'nltk.', 'hope', 'become', 'expert', 'nltk.', 'know', 'need', 'lot', 'efforts', 'end', 'really', 'want', 'this.', 'This', 'add', 'lot', 'confidence', 'play', 'role', 'DS']\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print(\" \")\n",
    "    print(\"----Reading Content of raw text-----\")\n",
    "    print(\" \")\n",
    "    \n",
    "    file_content=fileread()\n",
    "    \n",
    "    sw=set(stopwords.words('english'))\n",
    "    \n",
    "    custom_sw=set(['would','I','this'])\n",
    "    print(custom_sw)\n",
    "    \n",
    "    final_text=[]\n",
    "    \n",
    "    for i in file_content.split():\n",
    "        if i not in sw | custom_sw:\n",
    "            final_text.append(i)\n",
    "    print(final_text)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Reading content of the raw text \n",
      " \n",
      "['this', 'is', 'a', 'test', 'file', 'for', 'learning', 'nltk', '.', 'I', 'hope', 'i', 'become', 'an', 'expert', 'in', 'nltk', '.', 'I', 'know', 'it', 'would', 'need', 'a', 'lot', 'of', 'efforts', 'from', 'my', 'end', 'but', 'i', 'really', 'want', 'to', 'do', 'this', '.', 'This', 'would', 'add', 'lot', 'of', 'confidence', 'in', 'me', 'to', 'play', 'a', 'role', 'of', 'DS']\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print(\" \")\n",
    "    print(\" Reading content of the raw text \")\n",
    "    print (\" \")\n",
    "    \n",
    "    file_content=fileread()\n",
    "    \n",
    "    words=word_tokenize(file_content)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
